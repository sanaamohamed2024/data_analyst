{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build the function of tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_path):\n",
    "    tokens = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the file\n",
    "        paragraph = file.read()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    paragraph = paragraph.lower()\n",
    "    # Remove all characters except letters, digits, and whitespace\n",
    "    paragraph = re.sub(r\"[^a-z0-9'\\s]\", '', paragraph)\n",
    "    \n",
    "    # using \\S+ to split the paragraph into words\n",
    "    tokens = re.findall(r'\\S+', paragraph)\n",
    "    \n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply the function on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the token= ['twitter', 'is', 'the', 'rice', 'of', 'social', 'media', 'rumour', 'has', 'it', 'targeted', 'online', 'advertising', 'was', 'developed', 'because', 'the', 'internet', 'was', 'upset', 'that', 'you', 'could', 'read', 'it', 'but', 'it', \"couldn't\", 'read', 'you', 'trepidelicious', 'a', 'tagline', 'for', 'an', 'airline', 'take', 'the', 'high', 'road', 'pantone', 'is', 'a', 'colour', 'but', 'also', 'the', 'singular', 'version', 'of', 'pants', 'smiling', 'could', 'easily', 'be', 'misinterpreted', 'for', 'showing', 'your', 'teeth', 'to', 'someone', 'because', 'they', 'said', 'something', 'that', 'made', 'you', 'happy', 'tim', 'horton', 'was', 'a', 'hockey', 'player', 'but', 'is', 'the', 'name', 'of', 'a', 'coffee', 'chain', 'which', 'means', 'my', 'dream', 'of', 'a', 'goat', 'sanctuary', 'being', 'my', 'legacy', 'is', 'not', 'unrealistic', 'i', \"don't\", 'need', 'a', 'big', 'house', 'just', 'a', 'twofloor', 'condo', 'you', 'could', 'say', 'i', 'have', 'lofty', 'expectations', \"i'm\", 'still', 'upset', 'that', 'tie', 'domi', \"didn't\", 'name', 'his', 'child', 'tyson', 'are', 'there', 'outofstock', 'photos', 'gafuffle', 'you', 'should', 'listen', 'to', 'my', 'mixtape', 'check', 'out', 'the', 'rest', 'of', 'my', 'portfolio', 'rumour', 'has', 'it', 'targeted', 'online', 'advertising', 'was', 'developed', 'because', 'the', 'internet', 'was', 'upset', 'that', 'you', 'could', 'read', 'it', 'but', 'it', \"couldn't\", 'read', 'you', 'trepidelicious', 'to', 'catch', 'a', 'predator', 'would', 'have', 'been', 'a', 'great', 'name', 'for', 'a', 'steve', 'irwin', 'show', 'mintslavicia', 'i', 'started', 'a', 'sensory', 'deprivation', 'chamber', 'business', 'it', 'involves', 'really', 'dark', 'curtains', 'ear', 'plugs', 'and', 'a', 'sleeping', 'mask', 'rumour', 'has', 'it', 'targeted', 'online', 'advertising', 'was', 'developed', 'because', 'the', 'internet', 'was', 'upset', 'that', 'you', 'could', 'read', 'it', 'but', 'it', \"couldn't\", 'read', 'you', 'trepidelicious', 'curling', 'is', 'the', 'best', 'sport', 'named', 'after', 'something', 'you', 'do', 'to', 'your', 'hair']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Example2.txt'  # Path to your file\n",
    "token= tokenize(file_path)\n",
    "print(\"the token=\",token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count the frequency of each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'developed': 3, 'house': 1, 'has': 3, 'pants': 1, 'photos': 1, 'media': 1, 'plugs': 1, 'steve': 1, 'it': 10, 'social': 1, 'tyson': 1, 'showing': 1, 'trepidelicious': 3, 'and': 1, 'is': 5, 'sleeping': 1, 'expectations': 1, 'but': 5, 'gafuffle': 1, 'catch': 1, 'airline': 1, 'curling': 1, 'for': 3, 'version': 1, 'legacy': 1, 'involves': 1, 'mask': 1, \"i'm\": 1, 'because': 4, 'an': 1, 'show': 1, 'there': 1, 'rest': 1, 'singular': 1, 'coffee': 1, 'have': 2, 'internet': 3, 'of': 5, 'listen': 1, 'read': 6, 'tagline': 1, 'business': 1, 'do': 1, 'say': 1, 'irwin': 1, 'easily': 1, 'rice': 1, 'out': 1, 'mixtape': 1, 'you': 10, 'would': 1, 'twitter': 1, 'outofstock': 1, 'still': 1, 'really': 1, 'that': 5, 'dream': 1, 'not': 1, 'lofty': 1, 'are': 1, 'hair': 1, 'a': 12, \"don't\": 1, 'tim': 1, 'sport': 1, 'i': 3, 'chain': 1, 'goat': 1, \"didn't\": 1, 'child': 1, 'advertising': 3, 'portfolio': 1, 'misinterpreted': 1, 'said': 1, 'could': 5, 'hockey': 1, 'be': 1, 'deprivation': 1, 'something': 2, 'dark': 1, 'take': 1, 'predator': 1, 'best': 1, 'sanctuary': 1, 'being': 1, 'need': 1, 'my': 4, 'was': 7, 'they': 1, 'online': 3, 'just': 1, 'started': 1, 'ear': 1, 'sensory': 1, 'big': 1, 'upset': 4, 'the': 9, 'road': 1, 'high': 1, 'check': 1, 'means': 1, 'smiling': 1, \"couldn't\": 3, 'horton': 1, 'colour': 1, 'curtains': 1, 'after': 1, 'named': 1, 'should': 1, 'rumour': 3, 'tie': 1, 'his': 1, 'great': 1, 'mintslavicia': 1, 'pantone': 1, 'unrealistic': 1, 'to': 4, 'twofloor': 1, 'targeted': 3, 'someone': 1, 'name': 3, 'teeth': 1, 'condo': 1, 'domi': 1, 'happy': 1, 'also': 1, 'been': 1, 'which': 1, 'made': 1, 'player': 1, 'your': 2, 'chamber': 1}\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set(token))\n",
    "    \n",
    "# Count the frequency of each word \n",
    "word_count = {}\n",
    "for word in unique_words:\n",
    "         word_count[word] = token.count(word)\n",
    " \n",
    "print(word_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
